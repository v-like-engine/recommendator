import pandas as pd
import torch

from benchmark.evaluate import evaluate_predictions
from models.glocal_k.networks import GLocalKFine
from models.utils.config import checkpoint_dir, project_path


def inference(data_matrix, best_checkpoint_filename):
    """
    Function to obtain an inference of glocal-k model
    :param data_matrix: data to predict on (train set)
    :param best_checkpoint_filename: filename of the model checkpoint you want to use. Essentially best one
    :return: numpy array representation of predictions (transposed originally)
    """
    glocal_k_fine = GLocalKFine.load_from_checkpoint(
        project_path + '/' + checkpoint_dir + '/' + best_checkpoint_filename
    )
    glocal_k_fine.eval()
    prediction = glocal_k_fine(torch.Tensor(data_matrix))
    return prediction.detach().numpy()


def evaluate_model(dataset, targets, best_checkpoint_filename):
    """
    Predict and evaluate predictions by glocal-k model
    :param dataset: data to predict on (train set)
    :param targets: test dataframe
    :param best_checkpoint_filename: filename of the model checkpoint you want to use. Essentially best one
    :return: dictionary of metrics calculated on test set
    """
    predicted_ratings = inference(dataset, best_checkpoint_filename)
    predicted_df = pd.DataFrame(predicted_ratings.T)
    predicted_df = predicted_df.rename(columns={i: i + 1 for i in range(len(predicted_df.columns))})
    return evaluate_predictions(predicted_df, targets)


def evaluate_preds(preds, targets):
    """
    simple function that takes predictions generated by the glocal-k model and targets, returning metrics calculated
    :param preds: predictions obtained from glocal-k
    :param targets: test dataframe
    :return: dictionary of metrics calculated on test set
    """
    predicted_df = pd.DataFrame(preds.T)
    predicted_df = predicted_df.rename(columns={i: i + 1 for i in range(len(predicted_df.columns))})
    return evaluate_predictions(predicted_df, targets)


def generate_recommendations(user_id, k, dataset, best_checkpoint_filename):
    """
    Function to generate top k recommendations for user user_id using the model loaded with best weights
    by inference on dataset
    :param user_id: id of user
    :param k: number of recommendations
    :param dataset: data to predict on (train set)
    :param best_checkpoint_filename: filename of the model checkpoint you want to use. Essentially best one
    :return: pd.DataFrame top k predictions (movie_id | predicted rating)
    """
    predicted_ratings = inference(dataset, best_checkpoint_filename)
    predicted_df = pd.DataFrame(predicted_ratings.T)
    predicted_df = predicted_df.rename(columns={i: i + 1 for i in range(len(predicted_df.columns))})
    user_ratings = predicted_df.loc[user_id - 1]
    user_ratings = user_ratings.sort_values(ascending=False)
    return user_ratings.head(k)
